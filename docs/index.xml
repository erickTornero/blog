<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research blog</title>
    <link>https://ericktornero.github.io/blog/</link>
    <description>Recent content on Research blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2020 15:39:27 -0500</lastBuildDate>
    
        <atom:link href="https://ericktornero.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Advances in Model Based Reinforcement Learning</title>
      <link>https://ericktornero.github.io/blog/posts/ambrl/</link>
      <pubDate>Thu, 30 Jan 2020 15:39:27 -0500</pubDate>
      
      <guid>https://ericktornero.github.io/blog/posts/ambrl/</guid>
      <description>&lt;p&gt;Introduction to the basics on model-based reinforcement learning and the recent advances on this topic.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Model-based reinforcement learning is a branch in Reinforcement Learning where the transition function &lt;span  class=&#34;math&#34;&gt;\(P(s&#39;, r|s, a)\)&lt;/span&gt; in the Markov decision process (MDP) is used. This model is used to improve or create the policy (planning), where the value function is calculated as an intermediate step, as can be seen in &lt;strong&gt;Figure 1&lt;/strong&gt; in the &lt;em&gt;model-learning&lt;/em&gt; branch. &lt;em&gt;Direct RL&lt;/em&gt;, means model-free RL where the policy or value-function is computed &lt;em&gt;directly&lt;/em&gt; from experience and it is not needed to model the dynamics. However, these kinds of methods are too poor sample efficient due to that these methods rely on the probability to find good interactions, this becomes even more complicated when the environment has high dimensional state-action space and when sparse rewards are presented.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/squeme_mbmf.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 1, Taken from [1]&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;The basic form of model-based reinforcement learning can be seen in &lt;strong&gt;Dynamic Programming&lt;/strong&gt;, which is assumed a prior knowledge over the dynamics or the transition function. However, in the real world, the dynamics are usually unknown and can be very complex to model. For these kinds of problems, model learning can be used just as supervised learning. In &lt;strong&gt;Fig. 2&lt;/strong&gt;, the left picture shows a Simple Gridworld example with a discrete state and actions. In this case the transition function is known, for example &lt;span  class=&#34;math&#34;&gt;\(P(s_2|s_1,right)=1.0\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(P(s_2|s_1, left)=0.0\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(s_x\)&lt;/span&gt;: represents the slot &lt;span  class=&#34;math&#34;&gt;\(x \in \{0, 15\}\)&lt;/span&gt;. In the right picture, Halfcheetah in the &lt;a href=&#34;http://www.mujoco.org/&#34;&gt;Mujoco Environment&lt;/a&gt; is shown, where a priory of the dynamics of this environment is unknown and complex.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/gridworld_hchetaah.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 2, left: Gridworld environment, taken from [1]. Right: Halfcheetah Environment&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;For low dimensional state-action space, Gaussian Process (GPs) can be used to approximate the transition function. However when complexity in the model increasses, e.g. in robotics control, gaussian process used to be inadequate. Neural Networks however are known by its high adaptavility to complex functions, and in recent years, has beend showed interesting results in several applications, such as image classification. In that sense, this post focused in recent advances in MBRL that uses Neural Networks for the approximation of the transition function.&lt;/p&gt;

&lt;h2 id=&#34;basic-concepts-in-modelbased-reinforcement-learnig&#34;&gt;Basic concepts in Model-Based Reinforcement Learnig&lt;/h2&gt;

&lt;p&gt;Reinforcement learning Framework is defined over a Markov Decision Process&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/mdp.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 3, Interaction in MDP&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;neural-network-dynamics-for-modelbased-deep-reinforcement-learning-with-modelfree-finetuning&#34;&gt;Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning&lt;/h2&gt;

&lt;h3 id=&#34;anusha-nagabandi-et-al-uc-berkeley-2017&#34;&gt;Anusha Nagabandi et al. UC Berkeley 2017&lt;/h3&gt;

&lt;h2 id=&#34;deep-reinforcement-learning-with-a-handful-of-trials-with-probabilistic-models&#34;&gt;Deep Reinforcement Learning with a handful of trials with probabilistic models&lt;/h2&gt;

&lt;p&gt;This is a resume of paper published in NeuriPS 2018 Montreal, we create a brief summary and the highlights of this paper&lt;/p&gt;

&lt;p&gt;This paper introduces uncertainty-aware to the dynamics model. In comparison&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[TD_x = e^2\]&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;deep-dynamics-models-for-learning-dexterous-manipulation&#34;&gt;Deep Dynamics Models for Learning Dexterous Manipulation&lt;/h2&gt;

&lt;h3 id=&#34;anusha-nagabandi-et-al-uc-berkeley-2019&#34;&gt;Anusha Nagabandi et al. UC Berkeley 2019&lt;/h3&gt;

&lt;h2 id=&#34;learning-latent-dynamics-for-planning-from-pixels&#34;&gt;Learning Latent Dynamics for Planning from Pixels&lt;/h2&gt;</description>
    </item>
    
  </channel>
</rss>