<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research blog</title>
    <link>https://ericktornero.github.io/blog/</link>
    <description>Recent content on Research blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2020 15:39:27 -0500</lastBuildDate>
    
        <atom:link href="https://ericktornero.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Advances in Model Based Reinforcement Learning</title>
      <link>https://ericktornero.github.io/blog/posts/ambrl/</link>
      <pubDate>Thu, 30 Jan 2020 15:39:27 -0500</pubDate>
      
      <guid>https://ericktornero.github.io/blog/posts/ambrl/</guid>
      <description>&lt;p&gt;Introduction to the basics on model-based reinforcement learning and the recent advances on this topic.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Model-based reinforcement learning is a branch in Reinforcement Learning where the transition function &lt;span  class=&#34;math&#34;&gt;\(P(s&#39;, r|s, a)\)&lt;/span&gt; in the Markov decision process (MDP) is used. This model is used to improve or create the policy (planning), where the value function is calculated as an intermediate step, as can be seen in &lt;strong&gt;Figure 1&lt;/strong&gt; in the &lt;em&gt;model-learning&lt;/em&gt; branch. &lt;em&gt;Direct RL&lt;/em&gt;, means model-free RL where the policy or value-function is computed &lt;em&gt;directly&lt;/em&gt; from experience and it is not needed to model the dynamics. However, these kinds of methods are too poor sample efficient due to that these methods rely on the probability to find good interactions, this becomes even more complicated when the environment has high dimensional state-action space and when sparse rewards are presented.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/squeme_mbmf.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 1, Taken from [1]&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;The basic form of model-based reinforcement learning can be seen in &lt;strong&gt;Dynamic Programming&lt;/strong&gt;, which is assumed a prior knowledge over the dynamics or the transition function. However, in the real world, the dynamics are usually unknown and can be very complex to model. For these kinds of problems, model learning can be used just as supervised learning. In &lt;strong&gt;Fig. 2&lt;/strong&gt;, the left picture shows a Simple Gridworld example with a discrete state and actions. In this case the transition function is known, for example &lt;span  class=&#34;math&#34;&gt;\(P(s_2|s_1,right)=1.0\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(P(s_2|s_1, left)=0.0\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(s_x\)&lt;/span&gt;: represents the slot &lt;span  class=&#34;math&#34;&gt;\(x \in \{0, 15\}\)&lt;/span&gt;. In the right picture, Halfcheetah in the &lt;a href=&#34;http://www.mujoco.org/&#34;&gt;Mujoco Environment&lt;/a&gt; is shown, where a priory of the dynamics of this environment is unknown and complex.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/gridworld_hchetaah.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 2, left: Gridworld environment, taken from [1]. Right: Halfcheetah Environment&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;For low dimensional state-action space, Gaussian Process (GPs) can be used to approximate the transition function. However when complexity in the model increasses, e.g. in robotics control, gaussian process used to be inadequate. Neural Networks however are known by its high adaptavility to complex functions, and in recent years, has beend showed interesting results in several applications, such as image classification. In that sense, this post focused in recent advances in MBRL that uses Neural Networks for the approximation of the transition function.&lt;/p&gt;

&lt;h2 id=&#34;basic-concepts-in-modelbased-reinforcement-learnig&#34;&gt;Basic concepts in Model-Based Reinforcement Learnig&lt;/h2&gt;

&lt;p&gt;Reinforcement learning Framework is defined over a Markov Decision Process, and elements in an MDP are defined in the tuple &lt;span  class=&#34;math&#34;&gt;\((\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma)\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(\mathcal{S}\)&lt;/span&gt; is the state space, &lt;span  class=&#34;math&#34;&gt;\(\mathcal{A}\)&lt;/span&gt; is the action space, &lt;span  class=&#34;math&#34;&gt;\(\mathcal{R}\)&lt;/span&gt; is the reward and &lt;span  class=&#34;math&#34;&gt;\(\mathcal{P(s&#39;, r|s, a)}\)&lt;/span&gt; define the transition function. In &lt;strong&gt;Fig. 3&lt;/strong&gt;, the interaction interaction agent-envrionment is shown, given an observed state &lt;span  class=&#34;math&#34;&gt;\(s_t \in \mathcal{S}\)&lt;/span&gt; at any particular timestep &lt;span  class=&#34;math&#34;&gt;\(t\)&lt;/span&gt;, the agent take an action &lt;span  class=&#34;math&#34;&gt;\(a_t \in \mathcal{A}\)&lt;/span&gt; drawn from its policy &lt;span  class=&#34;math&#34;&gt;\(\pi(a_t|s_t)\)&lt;/span&gt;. The environment respond with the next state &lt;span  class=&#34;math&#34;&gt;\(s_{t+1}\)&lt;/span&gt; and the reward &lt;span  class=&#34;math&#34;&gt;\(r_{t+1}\)&lt;/span&gt; produced by take action &lt;span  class=&#34;math&#34;&gt;\(a_t\)&lt;/span&gt;.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/mdp.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 3, Interaction in MDP&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;Either model-based or model-free methods try to compute a &lt;strong&gt;policy&lt;/strong&gt; &lt;span  class=&#34;math&#34;&gt;\(\pi(a|s)\)&lt;/span&gt; that maximizes the expected reward. In the case of mode-based, the policy is computed indirectly by using the transition model &lt;span  class=&#34;math&#34;&gt;\(\mathcal{P}(s&#39;,r|s, a)\)&lt;/span&gt; for planning. As an example, some algorithms are explaining in the following section called: &lt;strong&gt;Dynamic Programming&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;dynamic-programming&#34;&gt;Dynamic Programming&lt;/h3&gt;

&lt;p&gt;Dynamic programming are sets of algorithms where is assumed that the environment transition function is priorly known. The aim of RL is summarized in the following equation:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\mathbb{E}[G_t] = \mathbb{E}_{\pi}[\sum_t \gamma^tr_t | s_t,a_t]\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\mathbb{E}[G_t] = \sum_s [\sum_t v(s) | a_t]\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[v_\pi(s) = \sum_{s, r} \sum_{a} p(s&#39;|s,a) \pi(a|s)[r + v(s)]\]&lt;/span&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;neural-network-dynamics-for-modelbased-deep-reinforcement-learning-with-modelfree-finetuning&#34;&gt;Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning&lt;/h2&gt;

&lt;h3 id=&#34;anusha-nagabandi-et-al-uc-berkeley-2017&#34;&gt;Anusha Nagabandi et al. UC Berkeley 2017&lt;/h3&gt;

&lt;p&gt;This paper proposes deterministic Neural Network for Model-based Reinforcement for solving continuous taks. The pipeline is shown in &lt;strong&gt;Fig. 4&lt;/strong&gt;, a Neural Network &lt;span  class=&#34;math&#34;&gt;\(\hat{f}_\theta\)&lt;/span&gt; models the dynamics of the environment and is trained as a multidimensional regression in a deterministic way using &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34;&gt;MSE Loss&lt;/a&gt; function (&lt;strong&gt;Eq. 3&lt;/strong&gt;). The &lt;em&gt;MPC Controller&lt;/em&gt; laverage the knowledge of the transition function &lt;span  class=&#34;math&#34;&gt;\(\hat{f}_{\theta}\)&lt;/span&gt; to predict the next state &lt;span  class=&#34;math&#34;&gt;\(\hat{s}_{t+1}\)&lt;/span&gt; given a current state &lt;span  class=&#34;math&#34;&gt;\(s_t\)&lt;/span&gt; and an action taken &lt;span  class=&#34;math&#34;&gt;\(a_t\)&lt;/span&gt; as in &lt;strong&gt;Eq. 1&lt;/strong&gt;. Then this used for multistep prediction of &lt;em&gt;horizon&lt;/em&gt; &lt;span  class=&#34;math&#34;&gt;\(H\)&lt;/span&gt; by recursively applying the &lt;strong&gt;Eq. 1&lt;/strong&gt; &lt;span  class=&#34;math&#34;&gt;\(H\)&lt;/span&gt; times (&lt;strong&gt;Eq. 2&lt;/strong&gt;). Note that reward is a function over states &lt;span  class=&#34;math&#34;&gt;\(r_t = fn(s_t)\)&lt;/span&gt;, then if the state &lt;span  class=&#34;math&#34;&gt;\(s_t\)&lt;/span&gt; is predicted by &lt;strong&gt;Eq. 1&lt;/strong&gt;, reward is approximate by &lt;span  class=&#34;math&#34;&gt;\(\hat{r}_{t+i} = fn(\hat{s}_{t+i})\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\hat{s}_{t+1} = s_t + \hat{f}_\theta(s_t, a_t)\tag{1}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\hat{s}_{t+i} = \hat{f}_\theta(\dots(\hat{f}_\theta(\hat{f}_\theta(s_t, a_t), a_{t+1})\dots, a_{t+i}) \tag{2}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[Loss_\theta = \frac{1}{\mathcal{D}} \sum_{(s_t, a_t, s_{t+1}) \in \mathcal{D}} \frac{1}{2} \Vert (s_{t+1} - s_t) - \hat{f}_\theta(s_t, a_t) \Vert^2 \tag{3}\]&lt;/span&gt;&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/anusha2017.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 4, Pipeline Anusha paper&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;This method reach aceptable results in continuous tasks, this was shown in the &lt;a href=&#34;http://www.mujoco.org/&#34;&gt;Mujoco Environment&lt;/a&gt;. While this method take over 100x less interactions with respect model-free methods, this method can not achieve the assimpotic results, to reduce this gap this method, uses the previous model-based for initialize a model-free method (&lt;em&gt;PPO&lt;/em&gt;), this is particular useful for model-free becouse model-free performance based on the probability to see good interactions, and model-based can achieve this with few samples, resulting in a faster convergence for model-free methods as can be seen in &lt;strong&gt;Fig. 5&lt;/strong&gt; and &lt;strong&gt;Fig. 6&lt;/strong&gt; (&lt;em&gt;fine-tuning&lt;/em&gt;).&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/anusha2017results1.gif&#34;
         alt=&#34;Figure 5, Performance of Model-based method (left) vs Model-free with fine tuning (right), aceptable performance is show for model-based with just thousands of samples, model-free shown high performance in the convergence (millions of samples).&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 5, Performance of Model-based method (left) vs Model-free with fine tuning (right), aceptable performance is show for model-based with just thousands of samples, model-free shown high performance in the convergence (millions of samples).&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/anusha2017results2.png&#34;
         alt=&#34;Figure 6, Performance Model-free with fine tuning (red) vs Model-free (blue). Here, greater sample efficient of applying fine-tuning over the previous model-based is shown. We can appreciate that is difficult or imposible for a simple model-free method to achieve the performance of a Model-based method with the same number of samples, this feature is taked in advantage to make fine-tuning&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 6, Performance Model-free with fine tuning (red) vs Model-free (blue). Here, greater sample efficient of applying fine-tuning over the previous model-based is shown. We can appreciate that is difficult or imposible for a simple model-free method to achieve the performance of a Model-based method with the same number of samples, this feature is taked in advantage to make fine-tuning&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;deep-reinforcement-learning-with-a-handful-of-trials-with-probabilistic-models&#34;&gt;Deep Reinforcement Learning with a handful of trials with probabilistic models&lt;/h2&gt;

&lt;p&gt;This paper achieve interesting results reducing the gap between model-free and model-based RL in assimptotic performance with 10x less sample iterations required, by mixing some ideas to model the uncertainty of the dynamics: These uncertainties are devide into two:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Aleatoric Uncertainty&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;This uncertainty is given by the stochasticity of the system, e.g. noise. This allows to model differents variances for differents states. While the distribution over states can be assumed any tractable distribution, this paper assumed a Gaussian Distribution over states. Given by:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\hat{f} = Pr(s_{t+1}|s_t, a_t) = \mathcal{N}(\mu_\theta(s_t, a_t), \Sigma{_\theta}(s_t,a_t))\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In that sense the loss function is given by:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[loss_{Gauss}(\theta) = \sum_{n=1}^N[\mu_\theta(s_n, a_n) - s_{n+1}]^\intercal \Sigma_\theta^{-1}(s_n, a_n)[\mu_\theta(s_n, a_n) - s_{n+1}] + \dots \newline \dots + \log\det\Sigma{_\theta}(s_n, a_n)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Epistemic Uncertainty&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;This uncertainty is given by the limitation of data, similar to a bayesian model. This is made by using a simple bootstrap of ensembles. In &lt;strong&gt;Fig. 4&lt;/strong&gt;, an example of two ensembles is shown (red and blue). In zones where there are data for training, two ensembles behaive very similar, but in zones where not, for example between the two markable zones of datapoints, each ensemble can take different behaivor, these differences represents an uncertainty due to the missing of data or &lt;em&gt;epistemic uncertainty&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/epistemic_unc.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 4, probabilistc Ensembles&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h4 id=&#34;uncertainty-propagation&#34;&gt;Uncertainty propagation:&lt;/h4&gt;

&lt;p&gt;Several methods exists for the propagation in next states, this paper uses &lt;strong&gt;particle filter&lt;/strong&gt; method or known as a &lt;em&gt;Sequential Monte Carlo&lt;/em&gt;,&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/pipeline_handful.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 5, probabilistc Ensembles&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;figure&gt;
    &lt;img src=&#34;https://ericktornero.github.io/blog/images/algorithm_handful.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Figure 6, probabilistc Ensembles&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;deep-dynamics-models-for-learning-dexterous-manipulation&#34;&gt;Deep Dynamics Models for Learning Dexterous Manipulation&lt;/h2&gt;

&lt;h3 id=&#34;anusha-nagabandi-et-al-uc-berkeley-2019&#34;&gt;Anusha Nagabandi et al. UC Berkeley 2019&lt;/h3&gt;

&lt;h2 id=&#34;learning-latent-dynamics-for-planning-from-pixels&#34;&gt;Learning Latent Dynamics for Planning from Pixels&lt;/h2&gt;</description>
    </item>
    
  </channel>
</rss>