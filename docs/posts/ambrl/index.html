<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    
    <title>Advances in Model Based Reinforcement Learning | Research blog</title>
    <meta name="description" content="">
    <meta name="author" content="">
    
    <link rel="apple-touch-icon" sizes="180x180" href=http://ericktornero.github.io/blog/apple-touch-icon.png>
    <link rel="icon" type="image/png" sizes="32x32" href=http://ericktornero.github.io/blog/favicon-32x32.png>
    <link rel="icon" type="image/png" sizes="16x16" href=http://ericktornero.github.io/blog/favicon-16x16.png>
    <link rel="manifest" href=http://ericktornero.github.io/blog/site.webmanifest>
    <link rel="mask-icon" href=http://ericktornero.github.io/blog/safari-pinned-tab.svg color="#00416a">
    <meta name="msapplication-TileColor" content="#00aba9">
    <meta name="theme-color" content="#ffffff">
    
    
    
    
    
    
    <link rel="authorization_endpoint" href=https://indieauth.com/auth />
    <link rel="token_endpoint" href=https://tokens.indieauth.com/token />
    
    
    
    
    <link rel="stylesheet" href=http://ericktornero.github.io/blog/css/fonts.css />
    <link rel="stylesheet" href=http://ericktornero.github.io/blog/css/style.css />
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <div id="sitelogo">
        <a class="glyph" alt="Home" href="http://ericktornero.github.io/blog/"><img src=http://ericktornero.github.io/blog/images/site-logo.svg alt="Site Logo" height="64px" width="64px"></a>
    </div>
    <header>
        <nav>
    
    <div id="page-nav">
        <div class="page-nav-item">
            <a href="http://ericktornero.github.io/blog/">Home</a>
        </div>
        
    </div>
</nav>
        
    </header>




<div id="content">
    <article class="h-entry">
        <header>
            <h1 class="post-title p-name">Advances in Model Based Reinforcement Learning</h1>
            
            <p class="post-date">Posted on
                <time class="dt-published" datetime="2020-01-30T15:39:27-05:00">
                30 January, 2020 at 15:39 -05
                </time> 
            </p>
            
            
        </header>
        <section class="content e-content">
            <p>Introduction to the basics on model-based reinforcement learning and the recent advances on this topic.</p>

<h2 id="introduction">Introduction</h2>

<p>Model-based reinforcement learning, can be understanding as a learning the transition function <span  class="math">\(P(s', r|s,a)\)</span> in the markov desicion process (MDP). Then this model can be used for planning to take actions which in intermediate states the value function is computed.</p>

<figure>
    <img src="https://ericktornero.github.io/blog/images/squeme_mbmg.png"
         alt="fig-mbmf-sq"/> <figcaption>
            <p>Figure 1. Interaction Model, Experience &amp; Value/Policy. Image taken from [1]</p>
        </figcaption>
</figure>


<p>Basic form of model-based reinforcement learning can be seen in <strong>Dynamic Programming</strong>, in which is assumed a prior knowladge over the dynamics or the transition function. As we will see later, in real world, the dynamics is usually unknown un can be very complex. For these kind of problems, model learning can be used just as supervised learning.</p>

<p>For low dimensional state-action space, Gaussian Process (GPs) can be used to approximate the transition function. However when complexity in the model increasses, e.g. in robotics control, gaussian process used to be inadequate. Neural Networks however are known by its high adaptavility to complex functions as in images, and in recent years, has beend showed interesting results in several applications, in that sense, this post focused in recent advances in MBRL that uses Neural Networks for the approximation of the transition function.</p>

<h2 id="basic-concepts-in-modelbased-reinforcement-learnig">Basic concepts in Model-Based Reinforcement Learnig</h2>

<h2 id="deep-reinforcement-learning-with-a-handful-of-trials-with-probabilistic-models">Deep Reinforcement Learning with a handful of trials with probabilistic models</h2>

<p>This is a resume of paper published in NeuriPS 2018 Montreal, we create a brief summary and the highlights of this paper</p>

<p>This paper introduces uncertainty-aware to the dynamics model. In comparison</p>

<p><span  class="math">\[TD_x = e^2\]</span></p>
        </section>
        <footer>
            <a class="permalink u-url" href="http://ericktornero.github.io/blog/posts/ambrl/"></a>
            
        </footer>
    </article>
</div>

    
    <div id="footer">
      
      <aside id="social">
  <div id="social-icons">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </div>
</aside>

      
    </div>


</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
</html>
